from torch.nn.utils.rnn import pad_sequence

import torch
from torch.nn.utils.rnn import pad_sequence

def collate_action_fn(batch):
    """
    H√†m collate cho DataLoader, x·ª≠ l√Ω padding cho `label_embeddings`, `object_features`,
    v√† ƒë·∫£m b·∫£o `action_embeddings` c√≥ d·∫°ng (batch_size, 1024).
    C≈©ng t·∫°o mask cho `object_features` v√† `label_embeddings`.
    """
    temporal_features, object_features, context_features, label_embeddings, action_embeddings, caption_tokens, caption_embeds = zip(*batch)

    # Stack c√°c ƒë·∫∑c tr∆∞ng theo batch
    temporal_features = torch.stack(temporal_features)  # (batch_size, seq_len, 512)
    context_features = torch.stack(context_features)    # (batch_size, seq_len, 512)

    # üîπ Padding cho `object_features` (variable number of objects)
    padded_objects = pad_sequence([torch.tensor(obj, dtype=torch.float32) for obj in object_features],
                                  batch_first=True, padding_value=0)  # (batch_size, max_num_objects, feature_dim)


    # üîπ Padding cho `label_embeddings`
    padded_labels = pad_sequence([torch.tensor(lbl, dtype=torch.float32) for lbl in label_embeddings], batch_first=True, padding_value=0)

    # üîπ `action_embeddings`: (batch_size, 1024)
    # action_embeddings = torch.stack([torch.tensor(a, dtype=torch.float32) for a in action_embeddings])
    action_embeddings = pad_sequence([torch.tensor(action, dtype=torch.float32) for action in action_embeddings],
                                  batch_first=True, padding_value=0)  # (batch_size, max_num_objects, feature_dim)

    # üîπ `caption_tokens` & `caption_embeds`
    caption_tokens = torch.stack([torch.tensor(a) for a in caption_tokens])
    caption_embeds = torch.stack([torch.tensor(a, dtype=torch.float32) for a in caption_embeds])

    return temporal_features, padded_objects, context_features, padded_labels, action_embeddings, caption_tokens, caption_embeds


def collate_val_fn(batch):
    """
    H√†m collate cho DataLoader, x·ª≠ l√Ω padding cho `object_features` v√† t·∫°o mask cho `object_features`.
    """
    temporal_features, object_features, context_features, caption_tokens = zip(*batch)

    # üîπ Stack c√°c ƒë·∫∑c tr∆∞ng th·ªùi gian v√† ng·ªØ c·∫£nh (gi·∫£ s·ª≠ c√πng shape)
    temporal_features = torch.stack(temporal_features)  # (batch_size, seq_len, 512)
    context_features = torch.stack(context_features)    # (batch_size, seq_len, 512)

    # üîπ Padding `object_features` (num_objects kh√°c nhau)
    padded_object_features = pad_sequence(
        [torch.tensor(obj, dtype=torch.float32) for obj in object_features],
        batch_first=True,
        padding_value=0
    )  # (batch_size, max_num_objects, feature_dim)
 
    # üîπ Caption tokens (gi·ªØ nguy√™n list, b·∫°n c√≥ th·ªÉ pad n·∫øu c·∫ßn th√™m x·ª≠ l√Ω sau)
    caption_tokens = [torch.tensor(a) for a in caption_tokens]
    # for a in caption_tokens:
    #     for b in a:
    #         print("b.shape", b.shape)

    return temporal_features, padded_object_features, context_features, caption_tokens

